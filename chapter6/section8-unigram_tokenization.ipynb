{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f8ea4b",
   "metadata": {},
   "source": [
    "# Unigram tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e9afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c26719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eassi/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112470b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function pre_tokenize_str:\n",
      "\n",
      "pre_tokenize_str(self, sequence) method of tokenizers.pre_tokenizers.Sequence instance\n",
      "    Pre tokenize the given string\n",
      "\n",
      "    This method provides a way to visualize the effect of a\n",
      "    :class:`~tokenizers.pre_tokenizers.PreTokenizer` but it does not keep track of the\n",
      "    alignment, nor does it provide all the capabilities of the\n",
      "    :class:`~tokenizers.PreTokenizedString`. If you need some of these, you can use\n",
      "    :meth:`~tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize`\n",
      "\n",
      "    Args:\n",
      "        sequence (:obj:`str`):\n",
      "            A string to pre-tokeize\n",
      "\n",
      "    Returns:\n",
      "        :obj:`List[Tuple[str, Offsets]]`:\n",
      "            A list of tuple with the pre-tokenized parts and their offsets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a649c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁The', (0, 3)),\n",
       " ('▁LLM', (4, 7)),\n",
       " ('▁is', (8, 10)),\n",
       " ('▁trained', (11, 18)),\n",
       " ('▁to', (19, 21)),\n",
       " ('▁predict', (22, 29)),\n",
       " ('▁the', (30, 33)),\n",
       " ('▁next', (34, 38)),\n",
       " ('▁word.', (39, 44))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"The LLM is trained to predict the next word.\"\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e25d4",
   "metadata": {},
   "source": [
    "## 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f0bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tis\tthe\tHugging\tFace\tCourse.\t\n",
      "This\tchapter\tis\tabout\ttokenization.\t\n",
      "This\tsection\tshows\tseveral\ttokenizer\talgorithms.\t\n",
      "Hopefully,\tyou\twill\tbe\table\tto\tunderstand\thow\tthey\tare\ttrained\tand\tgenerate\ttokens.\t\n"
     ]
    }
   ],
   "source": [
    "for text in corpus:\n",
    "    # print(text)\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    # print(words_with_offsets)\n",
    "    for _, (start, end) in words_with_offsets:\n",
    "        print(text[start:end], end=\"\\t\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b64e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    for word, _ in words_with_offsets:\n",
    "        word_freqs[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef5a150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁Course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df891f1",
   "metadata": {},
   "source": [
    "## 初始化词汇表\n",
    "\n",
    "然后，我们需要将我们的词汇表初始化为比最终所需词汇表大小更大的值。我们必须包含所有基本字符（否则我们将无法对每个单词进行分词），但对于较大的子字符串，我们只会保留最常见的那些，因此我们按频率对它们进行排序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb759e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: ▁This\n",
      "subwords: ['▁', '▁T', '▁Th', '▁Thi', '▁This', 'T', 'Th', 'Thi', 'This', 'h', 'hi', 'his', 'i', 'is', 's']\n"
     ]
    }
   ],
   "source": [
    "for word, freq in word_freqs.items():\n",
    "    print(f\"word: {word}\")\n",
    "    subwords = []\n",
    "    for i in range(len(word)):\n",
    "        for j in range(i + 1, len(word) + 1):  # range函数的入参构成的区间是左闭右开的：[start, stop)，所以 len(word)还要加1\n",
    "            subwords.append(word[i:j])\n",
    "    print(f\"subwords: {subwords}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eafef36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁t', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('▁a', 5),\n",
       " ('▁to', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('▁T', 3),\n",
       " ('▁Th', 3),\n",
       " ('▁Thi', 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411929a",
   "metadata": {},
   "source": [
    "我们将字符与最佳字词组合起来，已得到一个包含300个词的初始词汇表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad922f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁', 31),\n",
       " ('T', 3),\n",
       " ('h', 9),\n",
       " ('i', 13),\n",
       " ('s', 13),\n",
       " ('t', 14),\n",
       " ('e', 21),\n",
       " ('H', 2),\n",
       " ('u', 6),\n",
       " ('g', 5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[:300 - len(char_freqs)]\n",
    "token_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0213235f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': 31,\n",
       " 'T': 3,\n",
       " 'h': 9,\n",
       " 'i': 13,\n",
       " 's': 13,\n",
       " 't': 14,\n",
       " 'e': 21,\n",
       " 'H': 2,\n",
       " 'u': 6,\n",
       " 'g': 5,\n",
       " 'n': 11,\n",
       " 'F': 1,\n",
       " 'a': 12,\n",
       " 'c': 3,\n",
       " 'C': 1,\n",
       " 'o': 13,\n",
       " 'r': 9,\n",
       " '.': 4,\n",
       " 'p': 2,\n",
       " 'b': 3,\n",
       " 'k': 3,\n",
       " 'z': 2,\n",
       " 'w': 3,\n",
       " 'v': 1,\n",
       " 'l': 7,\n",
       " 'm': 1,\n",
       " 'f': 1,\n",
       " 'y': 3,\n",
       " ',': 1,\n",
       " 'd': 4,\n",
       " '▁t': 7,\n",
       " 'is': 5,\n",
       " 'er': 5,\n",
       " '▁a': 5,\n",
       " '▁to': 4,\n",
       " 'to': 4,\n",
       " 'en': 4,\n",
       " '▁T': 3,\n",
       " '▁Th': 3,\n",
       " '▁Thi': 3,\n",
       " '▁This': 3,\n",
       " 'Th': 3,\n",
       " 'Thi': 3,\n",
       " 'This': 3,\n",
       " 'hi': 3,\n",
       " 'his': 3,\n",
       " 'th': 3,\n",
       " 'ou': 3,\n",
       " 'se': 3,\n",
       " '▁tok': 3,\n",
       " '▁toke': 3,\n",
       " '▁token': 3,\n",
       " 'tok': 3,\n",
       " 'toke': 3,\n",
       " 'token': 3,\n",
       " 'ok': 3,\n",
       " 'oke': 3,\n",
       " 'oken': 3,\n",
       " 'ke': 3,\n",
       " 'ken': 3,\n",
       " '▁s': 3,\n",
       " 'ra': 3,\n",
       " 'nd': 3,\n",
       " '▁i': 2,\n",
       " '▁is': 2,\n",
       " '▁th': 2,\n",
       " '▁the': 2,\n",
       " 'the': 2,\n",
       " 'he': 2,\n",
       " '▁H': 2,\n",
       " 'in': 2,\n",
       " 'rs': 2,\n",
       " 'te': 2,\n",
       " '▁ab': 2,\n",
       " 'ab': 2,\n",
       " '▁tokeni': 2,\n",
       " '▁tokeniz': 2,\n",
       " 'tokeni': 2,\n",
       " 'tokeniz': 2,\n",
       " 'okeni': 2,\n",
       " 'okeniz': 2,\n",
       " 'keni': 2,\n",
       " 'keniz': 2,\n",
       " 'eni': 2,\n",
       " 'eniz': 2,\n",
       " 'ni': 2,\n",
       " 'niz': 2,\n",
       " 'iz': 2,\n",
       " 'at': 2,\n",
       " 'ti': 2,\n",
       " 'tio': 2,\n",
       " 'tion': 2,\n",
       " 'io': 2,\n",
       " 'ion': 2,\n",
       " 'on': 2,\n",
       " '▁se': 2,\n",
       " 'ho': 2,\n",
       " 'how': 2,\n",
       " 'ow': 2,\n",
       " 'era': 2,\n",
       " 'al': 2,\n",
       " 's.': 2,\n",
       " 'll': 2,\n",
       " 'an': 2,\n",
       " 'and': 2,\n",
       " 'ne': 2,\n",
       " '▁Hu': 1,\n",
       " '▁Hug': 1,\n",
       " '▁Hugg': 1,\n",
       " '▁Huggi': 1,\n",
       " '▁Huggin': 1,\n",
       " '▁Hugging': 1,\n",
       " 'Hu': 1,\n",
       " 'Hug': 1,\n",
       " 'Hugg': 1,\n",
       " 'Huggi': 1,\n",
       " 'Huggin': 1,\n",
       " 'Hugging': 1,\n",
       " 'ug': 1,\n",
       " 'ugg': 1,\n",
       " 'uggi': 1,\n",
       " 'uggin': 1,\n",
       " 'ugging': 1,\n",
       " 'gg': 1,\n",
       " 'ggi': 1,\n",
       " 'ggin': 1,\n",
       " 'gging': 1,\n",
       " 'gi': 1,\n",
       " 'gin': 1,\n",
       " 'ging': 1,\n",
       " 'ing': 1,\n",
       " 'ng': 1,\n",
       " '▁F': 1,\n",
       " '▁Fa': 1,\n",
       " '▁Fac': 1,\n",
       " '▁Face': 1,\n",
       " 'Fa': 1,\n",
       " 'Fac': 1,\n",
       " 'Face': 1,\n",
       " 'ac': 1,\n",
       " 'ace': 1,\n",
       " 'ce': 1,\n",
       " '▁C': 1,\n",
       " '▁Co': 1,\n",
       " '▁Cou': 1,\n",
       " '▁Cour': 1,\n",
       " '▁Cours': 1,\n",
       " '▁Course': 1,\n",
       " '▁Course.': 1,\n",
       " 'Co': 1,\n",
       " 'Cou': 1,\n",
       " 'Cour': 1,\n",
       " 'Cours': 1,\n",
       " 'Course': 1,\n",
       " 'Course.': 1,\n",
       " 'our': 1,\n",
       " 'ours': 1,\n",
       " 'ourse': 1,\n",
       " 'ourse.': 1,\n",
       " 'ur': 1,\n",
       " 'urs': 1,\n",
       " 'urse': 1,\n",
       " 'urse.': 1,\n",
       " 'rse': 1,\n",
       " 'rse.': 1,\n",
       " 'se.': 1,\n",
       " 'e.': 1,\n",
       " '▁c': 1,\n",
       " '▁ch': 1,\n",
       " '▁cha': 1,\n",
       " '▁chap': 1,\n",
       " '▁chapt': 1,\n",
       " '▁chapte': 1,\n",
       " '▁chapter': 1,\n",
       " 'ch': 1,\n",
       " 'cha': 1,\n",
       " 'chap': 1,\n",
       " 'chapt': 1,\n",
       " 'chapte': 1,\n",
       " 'chapter': 1,\n",
       " 'ha': 1,\n",
       " 'hap': 1,\n",
       " 'hapt': 1,\n",
       " 'hapte': 1,\n",
       " 'hapter': 1,\n",
       " 'ap': 1,\n",
       " 'apt': 1,\n",
       " 'apte': 1,\n",
       " 'apter': 1,\n",
       " 'pt': 1,\n",
       " 'pte': 1,\n",
       " 'pter': 1,\n",
       " 'ter': 1,\n",
       " '▁abo': 1,\n",
       " '▁abou': 1,\n",
       " '▁about': 1,\n",
       " 'abo': 1,\n",
       " 'abou': 1,\n",
       " 'about': 1,\n",
       " 'bo': 1,\n",
       " 'bou': 1,\n",
       " 'bout': 1,\n",
       " 'out': 1,\n",
       " 'ut': 1,\n",
       " '▁tokeniza': 1,\n",
       " '▁tokenizat': 1,\n",
       " '▁tokenizati': 1,\n",
       " '▁tokenizatio': 1,\n",
       " '▁tokenization': 1,\n",
       " '▁tokenization.': 1,\n",
       " 'tokeniza': 1,\n",
       " 'tokenizat': 1,\n",
       " 'tokenizati': 1,\n",
       " 'tokenizatio': 1,\n",
       " 'tokenization': 1,\n",
       " 'tokenization.': 1,\n",
       " 'okeniza': 1,\n",
       " 'okenizat': 1,\n",
       " 'okenizati': 1,\n",
       " 'okenizatio': 1,\n",
       " 'okenization': 1,\n",
       " 'okenization.': 1,\n",
       " 'keniza': 1,\n",
       " 'kenizat': 1,\n",
       " 'kenizati': 1,\n",
       " 'kenizatio': 1,\n",
       " 'kenization': 1,\n",
       " 'kenization.': 1,\n",
       " 'eniza': 1,\n",
       " 'enizat': 1,\n",
       " 'enizati': 1,\n",
       " 'enizatio': 1,\n",
       " 'enization': 1,\n",
       " 'enization.': 1,\n",
       " 'niza': 1,\n",
       " 'nizat': 1,\n",
       " 'nizati': 1,\n",
       " 'nizatio': 1,\n",
       " 'nization': 1,\n",
       " 'nization.': 1,\n",
       " 'iza': 1,\n",
       " 'izat': 1,\n",
       " 'izati': 1,\n",
       " 'izatio': 1,\n",
       " 'ization': 1,\n",
       " 'ization.': 1,\n",
       " 'za': 1,\n",
       " 'zat': 1,\n",
       " 'zati': 1,\n",
       " 'zatio': 1,\n",
       " 'zation': 1,\n",
       " 'zation.': 1,\n",
       " 'ati': 1,\n",
       " 'atio': 1,\n",
       " 'ation': 1,\n",
       " 'ation.': 1,\n",
       " 'tion.': 1,\n",
       " 'ion.': 1,\n",
       " 'on.': 1,\n",
       " 'n.': 1,\n",
       " '▁sec': 1,\n",
       " '▁sect': 1,\n",
       " '▁secti': 1,\n",
       " '▁sectio': 1,\n",
       " '▁section': 1,\n",
       " 'sec': 1,\n",
       " 'sect': 1,\n",
       " 'secti': 1,\n",
       " 'sectio': 1,\n",
       " 'section': 1,\n",
       " 'ec': 1,\n",
       " 'ect': 1,\n",
       " 'ecti': 1,\n",
       " 'ectio': 1,\n",
       " 'ection': 1,\n",
       " 'ct': 1,\n",
       " 'cti': 1,\n",
       " 'ctio': 1,\n",
       " 'ction': 1,\n",
       " '▁sh': 1,\n",
       " '▁sho': 1,\n",
       " '▁show': 1,\n",
       " '▁shows': 1,\n",
       " 'sh': 1,\n",
       " 'sho': 1,\n",
       " 'show': 1,\n",
       " 'shows': 1,\n",
       " 'hows': 1,\n",
       " 'ows': 1,\n",
       " 'ws': 1,\n",
       " '▁sev': 1,\n",
       " '▁seve': 1,\n",
       " '▁sever': 1,\n",
       " '▁severa': 1,\n",
       " '▁several': 1,\n",
       " 'sev': 1,\n",
       " 'seve': 1,\n",
       " 'sever': 1,\n",
       " 'severa': 1,\n",
       " 'several': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs = {token: freq for token, freq in token_freqs}\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec83ee",
   "metadata": {},
   "source": [
    "## 初始化模型\n",
    "\n",
    "接下来，我们计算所有频率的总和，将频率转换为概率。然而，对于我们的分词模型，我们将会在它里面存储概率的对数，因为累加对数相比累乘小数在数值上更加稳定，同时，也会简化模型损失的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5654929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum(freq for _, freq in token_freqs.items())\n",
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96035d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': 2.952892114877499,\n",
       " 'T': 5.288267030694535,\n",
       " 'h': 4.189654742026425,\n",
       " 'i': 3.821929961901108,\n",
       " 's': 3.821929961901108,\n",
       " 't': 3.7478219897473863,\n",
       " 'e': 3.342356881639222,\n",
       " 'H': 5.6937321388027,\n",
       " 'u': 4.59511985013459,\n",
       " 'g': 4.777441406928545,\n",
       " 'n': 3.9889840465642745,\n",
       " 'F': 6.386879319362645,\n",
       " 'a': 3.9019726695746444,\n",
       " 'c': 5.288267030694535,\n",
       " 'C': 6.386879319362645,\n",
       " 'o': 3.821929961901108,\n",
       " 'r': 4.189654742026425,\n",
       " '.': 5.000584958242754,\n",
       " 'p': 5.6937321388027,\n",
       " 'b': 5.288267030694535,\n",
       " 'k': 5.288267030694535,\n",
       " 'z': 5.6937321388027,\n",
       " 'w': 5.288267030694535,\n",
       " 'v': 6.386879319362645,\n",
       " 'l': 4.440969170307332,\n",
       " 'm': 6.386879319362645,\n",
       " 'f': 6.386879319362645,\n",
       " 'y': 5.288267030694535,\n",
       " ',': 6.386879319362645,\n",
       " 'd': 5.000584958242754,\n",
       " '▁t': 4.440969170307332,\n",
       " 'is': 4.777441406928545,\n",
       " 'er': 4.777441406928545,\n",
       " '▁a': 4.777441406928545,\n",
       " '▁to': 5.000584958242754,\n",
       " 'to': 5.000584958242754,\n",
       " 'en': 5.000584958242754,\n",
       " '▁T': 5.288267030694535,\n",
       " '▁Th': 5.288267030694535,\n",
       " '▁Thi': 5.288267030694535,\n",
       " '▁This': 5.288267030694535,\n",
       " 'Th': 5.288267030694535,\n",
       " 'Thi': 5.288267030694535,\n",
       " 'This': 5.288267030694535,\n",
       " 'hi': 5.288267030694535,\n",
       " 'his': 5.288267030694535,\n",
       " 'th': 5.288267030694535,\n",
       " 'ou': 5.288267030694535,\n",
       " 'se': 5.288267030694535,\n",
       " '▁tok': 5.288267030694535,\n",
       " '▁toke': 5.288267030694535,\n",
       " '▁token': 5.288267030694535,\n",
       " 'tok': 5.288267030694535,\n",
       " 'toke': 5.288267030694535,\n",
       " 'token': 5.288267030694535,\n",
       " 'ok': 5.288267030694535,\n",
       " 'oke': 5.288267030694535,\n",
       " 'oken': 5.288267030694535,\n",
       " 'ke': 5.288267030694535,\n",
       " 'ken': 5.288267030694535,\n",
       " '▁s': 5.288267030694535,\n",
       " 'ra': 5.288267030694535,\n",
       " 'nd': 5.288267030694535,\n",
       " '▁i': 5.6937321388027,\n",
       " '▁is': 5.6937321388027,\n",
       " '▁th': 5.6937321388027,\n",
       " '▁the': 5.6937321388027,\n",
       " 'the': 5.6937321388027,\n",
       " 'he': 5.6937321388027,\n",
       " '▁H': 5.6937321388027,\n",
       " 'in': 5.6937321388027,\n",
       " 'rs': 5.6937321388027,\n",
       " 'te': 5.6937321388027,\n",
       " '▁ab': 5.6937321388027,\n",
       " 'ab': 5.6937321388027,\n",
       " '▁tokeni': 5.6937321388027,\n",
       " '▁tokeniz': 5.6937321388027,\n",
       " 'tokeni': 5.6937321388027,\n",
       " 'tokeniz': 5.6937321388027,\n",
       " 'okeni': 5.6937321388027,\n",
       " 'okeniz': 5.6937321388027,\n",
       " 'keni': 5.6937321388027,\n",
       " 'keniz': 5.6937321388027,\n",
       " 'eni': 5.6937321388027,\n",
       " 'eniz': 5.6937321388027,\n",
       " 'ni': 5.6937321388027,\n",
       " 'niz': 5.6937321388027,\n",
       " 'iz': 5.6937321388027,\n",
       " 'at': 5.6937321388027,\n",
       " 'ti': 5.6937321388027,\n",
       " 'tio': 5.6937321388027,\n",
       " 'tion': 5.6937321388027,\n",
       " 'io': 5.6937321388027,\n",
       " 'ion': 5.6937321388027,\n",
       " 'on': 5.6937321388027,\n",
       " '▁se': 5.6937321388027,\n",
       " 'ho': 5.6937321388027,\n",
       " 'how': 5.6937321388027,\n",
       " 'ow': 5.6937321388027,\n",
       " 'era': 5.6937321388027,\n",
       " 'al': 5.6937321388027,\n",
       " 's.': 5.6937321388027,\n",
       " 'll': 5.6937321388027,\n",
       " 'an': 5.6937321388027,\n",
       " 'and': 5.6937321388027,\n",
       " 'ne': 5.6937321388027,\n",
       " '▁Hu': 6.386879319362645,\n",
       " '▁Hug': 6.386879319362645,\n",
       " '▁Hugg': 6.386879319362645,\n",
       " '▁Huggi': 6.386879319362645,\n",
       " '▁Huggin': 6.386879319362645,\n",
       " '▁Hugging': 6.386879319362645,\n",
       " 'Hu': 6.386879319362645,\n",
       " 'Hug': 6.386879319362645,\n",
       " 'Hugg': 6.386879319362645,\n",
       " 'Huggi': 6.386879319362645,\n",
       " 'Huggin': 6.386879319362645,\n",
       " 'Hugging': 6.386879319362645,\n",
       " 'ug': 6.386879319362645,\n",
       " 'ugg': 6.386879319362645,\n",
       " 'uggi': 6.386879319362645,\n",
       " 'uggin': 6.386879319362645,\n",
       " 'ugging': 6.386879319362645,\n",
       " 'gg': 6.386879319362645,\n",
       " 'ggi': 6.386879319362645,\n",
       " 'ggin': 6.386879319362645,\n",
       " 'gging': 6.386879319362645,\n",
       " 'gi': 6.386879319362645,\n",
       " 'gin': 6.386879319362645,\n",
       " 'ging': 6.386879319362645,\n",
       " 'ing': 6.386879319362645,\n",
       " 'ng': 6.386879319362645,\n",
       " '▁F': 6.386879319362645,\n",
       " '▁Fa': 6.386879319362645,\n",
       " '▁Fac': 6.386879319362645,\n",
       " '▁Face': 6.386879319362645,\n",
       " 'Fa': 6.386879319362645,\n",
       " 'Fac': 6.386879319362645,\n",
       " 'Face': 6.386879319362645,\n",
       " 'ac': 6.386879319362645,\n",
       " 'ace': 6.386879319362645,\n",
       " 'ce': 6.386879319362645,\n",
       " '▁C': 6.386879319362645,\n",
       " '▁Co': 6.386879319362645,\n",
       " '▁Cou': 6.386879319362645,\n",
       " '▁Cour': 6.386879319362645,\n",
       " '▁Cours': 6.386879319362645,\n",
       " '▁Course': 6.386879319362645,\n",
       " '▁Course.': 6.386879319362645,\n",
       " 'Co': 6.386879319362645,\n",
       " 'Cou': 6.386879319362645,\n",
       " 'Cour': 6.386879319362645,\n",
       " 'Cours': 6.386879319362645,\n",
       " 'Course': 6.386879319362645,\n",
       " 'Course.': 6.386879319362645,\n",
       " 'our': 6.386879319362645,\n",
       " 'ours': 6.386879319362645,\n",
       " 'ourse': 6.386879319362645,\n",
       " 'ourse.': 6.386879319362645,\n",
       " 'ur': 6.386879319362645,\n",
       " 'urs': 6.386879319362645,\n",
       " 'urse': 6.386879319362645,\n",
       " 'urse.': 6.386879319362645,\n",
       " 'rse': 6.386879319362645,\n",
       " 'rse.': 6.386879319362645,\n",
       " 'se.': 6.386879319362645,\n",
       " 'e.': 6.386879319362645,\n",
       " '▁c': 6.386879319362645,\n",
       " '▁ch': 6.386879319362645,\n",
       " '▁cha': 6.386879319362645,\n",
       " '▁chap': 6.386879319362645,\n",
       " '▁chapt': 6.386879319362645,\n",
       " '▁chapte': 6.386879319362645,\n",
       " '▁chapter': 6.386879319362645,\n",
       " 'ch': 6.386879319362645,\n",
       " 'cha': 6.386879319362645,\n",
       " 'chap': 6.386879319362645,\n",
       " 'chapt': 6.386879319362645,\n",
       " 'chapte': 6.386879319362645,\n",
       " 'chapter': 6.386879319362645,\n",
       " 'ha': 6.386879319362645,\n",
       " 'hap': 6.386879319362645,\n",
       " 'hapt': 6.386879319362645,\n",
       " 'hapte': 6.386879319362645,\n",
       " 'hapter': 6.386879319362645,\n",
       " 'ap': 6.386879319362645,\n",
       " 'apt': 6.386879319362645,\n",
       " 'apte': 6.386879319362645,\n",
       " 'apter': 6.386879319362645,\n",
       " 'pt': 6.386879319362645,\n",
       " 'pte': 6.386879319362645,\n",
       " 'pter': 6.386879319362645,\n",
       " 'ter': 6.386879319362645,\n",
       " '▁abo': 6.386879319362645,\n",
       " '▁abou': 6.386879319362645,\n",
       " '▁about': 6.386879319362645,\n",
       " 'abo': 6.386879319362645,\n",
       " 'abou': 6.386879319362645,\n",
       " 'about': 6.386879319362645,\n",
       " 'bo': 6.386879319362645,\n",
       " 'bou': 6.386879319362645,\n",
       " 'bout': 6.386879319362645,\n",
       " 'out': 6.386879319362645,\n",
       " 'ut': 6.386879319362645,\n",
       " '▁tokeniza': 6.386879319362645,\n",
       " '▁tokenizat': 6.386879319362645,\n",
       " '▁tokenizati': 6.386879319362645,\n",
       " '▁tokenizatio': 6.386879319362645,\n",
       " '▁tokenization': 6.386879319362645,\n",
       " '▁tokenization.': 6.386879319362645,\n",
       " 'tokeniza': 6.386879319362645,\n",
       " 'tokenizat': 6.386879319362645,\n",
       " 'tokenizati': 6.386879319362645,\n",
       " 'tokenizatio': 6.386879319362645,\n",
       " 'tokenization': 6.386879319362645,\n",
       " 'tokenization.': 6.386879319362645,\n",
       " 'okeniza': 6.386879319362645,\n",
       " 'okenizat': 6.386879319362645,\n",
       " 'okenizati': 6.386879319362645,\n",
       " 'okenizatio': 6.386879319362645,\n",
       " 'okenization': 6.386879319362645,\n",
       " 'okenization.': 6.386879319362645,\n",
       " 'keniza': 6.386879319362645,\n",
       " 'kenizat': 6.386879319362645,\n",
       " 'kenizati': 6.386879319362645,\n",
       " 'kenizatio': 6.386879319362645,\n",
       " 'kenization': 6.386879319362645,\n",
       " 'kenization.': 6.386879319362645,\n",
       " 'eniza': 6.386879319362645,\n",
       " 'enizat': 6.386879319362645,\n",
       " 'enizati': 6.386879319362645,\n",
       " 'enizatio': 6.386879319362645,\n",
       " 'enization': 6.386879319362645,\n",
       " 'enization.': 6.386879319362645,\n",
       " 'niza': 6.386879319362645,\n",
       " 'nizat': 6.386879319362645,\n",
       " 'nizati': 6.386879319362645,\n",
       " 'nizatio': 6.386879319362645,\n",
       " 'nization': 6.386879319362645,\n",
       " 'nization.': 6.386879319362645,\n",
       " 'iza': 6.386879319362645,\n",
       " 'izat': 6.386879319362645,\n",
       " 'izati': 6.386879319362645,\n",
       " 'izatio': 6.386879319362645,\n",
       " 'ization': 6.386879319362645,\n",
       " 'ization.': 6.386879319362645,\n",
       " 'za': 6.386879319362645,\n",
       " 'zat': 6.386879319362645,\n",
       " 'zati': 6.386879319362645,\n",
       " 'zatio': 6.386879319362645,\n",
       " 'zation': 6.386879319362645,\n",
       " 'zation.': 6.386879319362645,\n",
       " 'ati': 6.386879319362645,\n",
       " 'atio': 6.386879319362645,\n",
       " 'ation': 6.386879319362645,\n",
       " 'ation.': 6.386879319362645,\n",
       " 'tion.': 6.386879319362645,\n",
       " 'ion.': 6.386879319362645,\n",
       " 'on.': 6.386879319362645,\n",
       " 'n.': 6.386879319362645,\n",
       " '▁sec': 6.386879319362645,\n",
       " '▁sect': 6.386879319362645,\n",
       " '▁secti': 6.386879319362645,\n",
       " '▁sectio': 6.386879319362645,\n",
       " '▁section': 6.386879319362645,\n",
       " 'sec': 6.386879319362645,\n",
       " 'sect': 6.386879319362645,\n",
       " 'secti': 6.386879319362645,\n",
       " 'sectio': 6.386879319362645,\n",
       " 'section': 6.386879319362645,\n",
       " 'ec': 6.386879319362645,\n",
       " 'ect': 6.386879319362645,\n",
       " 'ecti': 6.386879319362645,\n",
       " 'ectio': 6.386879319362645,\n",
       " 'ection': 6.386879319362645,\n",
       " 'ct': 6.386879319362645,\n",
       " 'cti': 6.386879319362645,\n",
       " 'ctio': 6.386879319362645,\n",
       " 'ction': 6.386879319362645,\n",
       " '▁sh': 6.386879319362645,\n",
       " '▁sho': 6.386879319362645,\n",
       " '▁show': 6.386879319362645,\n",
       " '▁shows': 6.386879319362645,\n",
       " 'sh': 6.386879319362645,\n",
       " 'sho': 6.386879319362645,\n",
       " 'show': 6.386879319362645,\n",
       " 'shows': 6.386879319362645,\n",
       " 'hows': 6.386879319362645,\n",
       " 'ows': 6.386879319362645,\n",
       " 'ws': 6.386879319362645,\n",
       " '▁sev': 6.386879319362645,\n",
       " '▁seve': 6.386879319362645,\n",
       " '▁sever': 6.386879319362645,\n",
       " '▁severa': 6.386879319362645,\n",
       " '▁several': 6.386879319362645,\n",
       " 'sev': 6.386879319362645,\n",
       " 'seve': 6.386879319362645,\n",
       " 'sever': 6.386879319362645,\n",
       " 'severa': 6.386879319362645,\n",
       " 'several': 6.386879319362645}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f0b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, loss in model.items():\n",
    "    assert loss >= 0, print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1446787b",
   "metadata": {},
   "source": [
    "## 维特比算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc28a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_word(word, model):\n",
    "#     best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "#         {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "#     ]  # 带有起点的词网（有向图），列表的每个元素代表词网的一个节点，start代表该节点的前驱节点，score代表起点到该节点的最短距离\n",
    "#     for start_idx in range(len(word)):\n",
    "#         # This should be properly filled by the previous steps of the loop\n",
    "#         best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "#         for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "#             token = word[start_idx:end_idx]\n",
    "#             if token in model and best_score_at_start is not None:\n",
    "#                 score = model[token] + best_score_at_start\n",
    "#                 # If we have found a better segmentation ending at end_idx, we update\n",
    "#                 if (\n",
    "#                     best_segmentations[end_idx][\"score\"] is None\n",
    "#                     or best_segmentations[end_idx][\"score\"] > score\n",
    "#                 ):\n",
    "#                     best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "#     segmentation = best_segmentations[-1]\n",
    "#     if segmentation[\"score\"] is None:\n",
    "#         # We did not find a tokenization of the word -> unknown\n",
    "#         return [\"<unk>\"], None\n",
    "\n",
    "#     score = segmentation[\"score\"]\n",
    "#     start = segmentation[\"start\"]\n",
    "#     end = len(word)\n",
    "#     tokens = []\n",
    "#     while start != 0:\n",
    "#         tokens.insert(0, word[start:end])\n",
    "#         next_start = best_segmentations[start][\"start\"]\n",
    "#         end = start\n",
    "#         start = next_start\n",
    "#     tokens.insert(0, word[start:end])\n",
    "#     return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5195b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_word(\"Hopefully\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b789326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import Dict\n",
    "\n",
    "def encode_word(word: str, model: Dict[str, float]):\n",
    "    n = len(word)\n",
    "    dp = [float(\"inf\")] * (n + 1)\n",
    "    pres = [None] * (n + 1)\n",
    "    dp[0] = 1\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n + 1):\n",
    "            # dp[j] = min(dp[j], dp[i] + get_dist(word, model, i, j))\n",
    "            dist = get_dist(word, model, i, j)\n",
    "            if dp[i] + dist < dp[j]:\n",
    "                dp[j] = dp[i] + dist\n",
    "                pres[j] = i\n",
    "    path = deque()\n",
    "    start, end = pres[-1], n\n",
    "    while start is not None:\n",
    "        token = word[start:end]\n",
    "        path.appendleft(token)\n",
    "        start, end = pres[start], start\n",
    "    tokens = [t for t in path]\n",
    "    best_score = dp[-1]\n",
    "    return tokens, best_score\n",
    "\n",
    "def get_dist(word: str, model: Dict[str, float], start: int, end: int):\n",
    "    token = word[start:end]\n",
    "    if token in model:\n",
    "        return model[token]\n",
    "    return float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c865643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_word(\"Hopefully\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ddc2e",
   "metadata": {},
   "source": [
    "## 计算整个语料库的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44fdece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def compute_losses(model: Dict[str, float]):\n",
    "    \"\"\" 计算整个语料库的损失值 \"\"\"\n",
    "    losses = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, loss = encode_word(word=word, model=model)\n",
    "        losses += freq * loss\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b27251",
   "metadata": {},
   "source": [
    "OK，使用初始化未经训练的模型来计算整个语料库的损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8014c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossed_initial = compute_losses(model=model)\n",
    "lossed_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6aa17",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26aa4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def compute_loss(model: Dict[str, float]):\n",
    "    \"\"\" 计算非字符子词的损失值，训练时将会移除低损失值的token \"\"\"\n",
    "    scores = {}  \n",
    "    losses_before = compute_losses(model)  # losses before removing token\n",
    "    for token, loss in model.items():\n",
    "        if len(token) == 1:  # 保留字符级的子词\n",
    "            continue\n",
    "        model_without_tokens = copy.deepcopy(model)\n",
    "        model_without_tokens.pop(token)\n",
    "        losses_after = compute_losses(model_without_tokens)  # losses after removing token\n",
    "        scores[token] = losses_after - losses_before\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9d262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_loss(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f60e3d",
   "metadata": {},
   "source": [
    "正式训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a106b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_loss(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    for i in range(int(percent_to_remove * len(model))):\n",
    "        token_freqs.pop(sorted_scores[i][0])\n",
    "    total_sum = sum(freq for _, freq in token_freqs.items())\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cd8b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364.2621620280587"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_trained = compute_losses(model)\n",
    "assert losses_trained < lossed_initial\n",
    "losses_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4552cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, loss in model.items():\n",
    "    assert loss >= 0, print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de557175",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f927aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[\"a\", \"b\", \"c\"]]\n",
    "sum([[\"a\", \"b\", \"c\"]], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b09fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    encoded_words = []\n",
    "    for word, _ in words_with_offsets:\n",
    "        encoded_word, _ = encode_word(word, model)\n",
    "        encoded_words.append(encoded_word)\n",
    "    # return [].extend(encoded_words)\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e80139d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁Hugging',\n",
       " '▁Face',\n",
       " '▁',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
