{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048ea33a",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding Tokenization (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aad838",
   "metadata": {},
   "source": [
    "## 1 语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586a85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20eb12e",
   "metadata": {},
   "source": [
    "## 2 预分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84afe69",
   "metadata": {},
   "source": [
    "BPE分词算法是被OpenAI用于预训练GPT系列模型而闻名的，由于我们将会手撸一个BPE分词器，所以我们需要先引入`gpt2`预训练模型对应的分词器来预分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9a9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eassi/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac2f913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'ĠHugging',\n",
       " 'ĠFace',\n",
       " 'ĠCourse',\n",
       " '.',\n",
       " 'This',\n",
       " 'Ġchapter',\n",
       " 'Ġis']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for sent in corpus:\n",
    "    for word, _ in tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sent):\n",
    "        words.append(word)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df5edf",
   "metadata": {},
   "source": [
    "注意，GPT-2的分词器采用字节级BPE(Byte-Level BPE)算法，将空格视为单词的一部分而非独立字符。为了区分“单词开头带空格”和“单词不带空格”的情况，分词器引入特殊符号 Ġ 作为空格的可视化表示，例如：\n",
    "- \"hello\" → 编码为 \"hello\"（无空格前缀）；\n",
    "- \" hello\" → 编码为 \"Ġhello\"（Ġ 表示前面的空格）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2238ba",
   "metadata": {},
   "source": [
    "## 3 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06b7141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'Ġis': 2,\n",
       "             'Ġthe': 1,\n",
       "             'ĠHugging': 1,\n",
       "             'ĠFace': 1,\n",
       "             'ĠCourse': 1,\n",
       "             '.': 4,\n",
       "             'Ġchapter': 1,\n",
       "             'Ġabout': 1,\n",
       "             'Ġtokenization': 1,\n",
       "             'Ġsection': 1,\n",
       "             'Ġshows': 1,\n",
       "             'Ġseveral': 1,\n",
       "             'Ġtokenizer': 1,\n",
       "             'Ġalgorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'Ġyou': 1,\n",
       "             'Ġwill': 1,\n",
       "             'Ġbe': 1,\n",
       "             'Ġable': 1,\n",
       "             'Ġto': 1,\n",
       "             'Ġunderstand': 1,\n",
       "             'Ġhow': 1,\n",
       "             'Ġthey': 1,\n",
       "             'Ġare': 1,\n",
       "             'Ġtrained': 1,\n",
       "             'Ġand': 1,\n",
       "             'Ġgenerate': 1,\n",
       "             'Ġtokens': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for word in words:\n",
    "    word_freqs[word] += 1\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98bd7db",
   "metadata": {},
   "source": [
    "## 4 初始化词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d156a4",
   "metadata": {},
   "source": [
    "按照BPE算法的约定，初始的词汇表由语料库中使用的所有字符以及预训练使用到的特殊字符组成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bd15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = []\n",
    "chars = set()\n",
    "\n",
    "for word in words:\n",
    "    for c in word:\n",
    "        if c not in chars:\n",
    "            chars.add(c)\n",
    "            alphabet.append(c)\n",
    "\n",
    "alphabet.sort()\n",
    "# alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b347a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_alphabet = [ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',\n",
    "  't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n",
    "assert alphabet == expected_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d24ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f85f0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = special_tokens + alphabet.copy()\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2fcaf",
   "metadata": {},
   "source": [
    "## 5 分割单词\n",
    "\n",
    "接下来我们需要将语料库中的所有单词分割为首字母以及为非首字母都加上“##”前缀："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97220e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', 'h', 'i', 's'],\n",
       " 'Ġis': ['Ġ', 'i', 's'],\n",
       " 'Ġthe': ['Ġ', 't', 'h', 'e'],\n",
       " 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
       " 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'],\n",
       " 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'],\n",
       " '.': ['.'],\n",
       " 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
       " 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'],\n",
       " 'Ġtokenization': ['Ġ',\n",
       "  't',\n",
       "  'o',\n",
       "  'k',\n",
       "  'e',\n",
       "  'n',\n",
       "  'i',\n",
       "  'z',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n'],\n",
       " 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'],\n",
       " 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
       " 'Ġtokenizer': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n",
       " 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " ',': [','],\n",
       " 'Ġyou': ['Ġ', 'y', 'o', 'u'],\n",
       " 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'],\n",
       " 'Ġbe': ['Ġ', 'b', 'e'],\n",
       " 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'],\n",
       " 'Ġto': ['Ġ', 't', 'o'],\n",
       " 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
       " 'Ġhow': ['Ġ', 'h', 'o', 'w'],\n",
       " 'Ġthey': ['Ġ', 't', 'h', 'e', 'y'],\n",
       " 'Ġare': ['Ġ', 'a', 'r', 'e'],\n",
       " 'Ġtrained': ['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
       " 'Ġand': ['Ġ', 'a', 'n', 'd'],\n",
       " 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " 'Ġtokens': ['Ġ', 't', 'o', 'k', 'e', 'n', 's']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {\n",
    "    word: [c for c in word]\n",
    "    for word in words\n",
    "}\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8334f78",
   "metadata": {},
   "source": [
    "## 6 计算分数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353a80",
   "metadata": {},
   "source": [
    "BPE通过合并出现频率最高的相邻字符对/子词对/词元对（这些词元对可以称为bigram pair）来不断扩充词汇表，直到词汇表的大小等于预先设置的期望大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fdb44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This: ('T', 'h'), ('h', 'i'), ('i', 's'), "
     ]
    }
   ],
   "source": [
    "def bigram():\n",
    "    for word, tokens in splits.items():\n",
    "        print(f\"{word}:\", end=\" \")\n",
    "        for i in range(len(tokens) - 1):\n",
    "            print(f\"{(tokens[i], tokens[i+1])}\", end=\", \")\n",
    "        break\n",
    "\n",
    "bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faeafa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bigram_freqs(word_freqs, splits):\n",
    "    bigram_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            # bigram_freqs[split[0]] += freq  # 坑死了\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            bigram = split[i], split[i+1]\n",
    "            bigram_freqs[bigram] += freq\n",
    "    return bigram_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3be52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ġ', 'i'): 2\n",
      "('Ġ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "freqs = compute_bigram_freqs(word_freqs, splits)\n",
    "for i, key in enumerate(freqs.keys()):\n",
    "    print(f\"{key}: {freqs[key]}\")\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbffb7f",
   "metadata": {},
   "source": [
    "## 7 合并子词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "215ba032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_pair(first, second, splits):\n",
    "#     for word, split in splits.items():\n",
    "#         if len(split) == 1:\n",
    "#             continue\n",
    "#         i = 0\n",
    "#         while i < len(split) - 1:\n",
    "#             if split[i] == first and split[i+1] == second:\n",
    "#                 pair = first + second[2:] if second.startswith(\"##\") else first + second\n",
    "#                 split = split[:i] + [pair] + split[i+2:]\n",
    "#             else:\n",
    "#                 i += 1\n",
    "#         splits[word] = split\n",
    "\n",
    "def merge_pair(first, second, splits):\n",
    "    for word, split in splits.items():\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        new_split = split.copy()\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == first and split[i+1] == second:\n",
    "                pair = first + second[2:] if second.startswith(\"##\") else first + second\n",
    "                new_split = split[:i] + [pair] + split[i+2:]\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = new_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141b3a0",
   "metadata": {},
   "source": [
    "## 8 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f6570",
   "metadata": {},
   "source": [
    "找到最大分数的Pair："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "698c26b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Ġ', 't'), 7),\n",
       " (('i', 's'), 5),\n",
       " (('e', 'r'), 5),\n",
       " (('Ġ', 'a'), 5),\n",
       " (('t', 'o'), 4)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_freqs = sorted(freqs.items(), key=lambda x: x[-1], reverse=True)\n",
    "sorted_freqs[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce8186",
   "metadata": {},
   "source": [
    "先合并('Ġ', 't')："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e42e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_rules = {(\"Ġ\", \"t\"): \"Ġt\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ee9580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt']\n"
     ]
    }
   ],
   "source": [
    "vocab.append(\"Ġt\")\n",
    "merge_pair(\"Ġ\", \"t\", splits)\n",
    "\n",
    "expected_split = ['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n",
    "assert splits[\"Ġtrained\"] == expected_split\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2242554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 's') 2\n",
      "('e', 'r') 2\n",
      "('Ġ', 'a') 2\n",
      "('Ġt', 'o') 2\n",
      "('e', 'n') 2\n",
      "('T', 'h') 2\n",
      "('Th', 'is') 2\n",
      "('o', 'u') 2\n",
      "('s', 'e') 2\n",
      "('Ġto', 'k') 2\n",
      "('Ġtok', 'en') 2\n",
      "('n', 'd') 2\n",
      "('Ġ', 'is') 2\n",
      "('Ġt', 'h') 2\n",
      "('Ġth', 'e') 2\n",
      "('i', 'n') 2\n",
      "('Ġa', 'b') 2\n",
      "('Ġtoken', 'i') 2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "while len(vocab) < vocab_size:\n",
    "    freqs = compute_bigram_freqs(word_freqs, splits)\n",
    "    sorted_freqs = sorted(freqs.items(), key=lambda x: x[-1], reverse=True)\n",
    "    best = sorted_freqs[0]\n",
    "    bigram, _ = best\n",
    "    print(bigram, len(bigram))\n",
    "    merge_pair(*bigram, splits)\n",
    "    token = bigram[0] + bigram[1]\n",
    "    merge_rules[bigram] = token\n",
    "    vocab.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab5842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_merge_rules = {('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en',\n",
    " ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok',\n",
    " ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe',\n",
    " ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e04f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert merge_rules == expected_merge_rules, print(f\"{merge_rules}\\n{vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab6d66",
   "metadata": {},
   "source": [
    "## 9 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ba831ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text, merge_rules):\n",
    "#     words = [word for word, _ in tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)]\n",
    "#     splits = {word: [c for c in word] for word in words}\n",
    "#     print(splits)\n",
    "#     for bigram in merge_rules.keys():\n",
    "#         first, second = bigram\n",
    "#         merge_pair(first, second, splits)\n",
    "#         print(f\"{bigram}: {splits}\")\n",
    "#     tokens = [splits[word] for word in words]\n",
    "#     return sum(tokens, [])\n",
    "\n",
    "def tokenize(text, merge_rules):\n",
    "    words = [word for word, _ in tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)]\n",
    "    splits = {word: [w for w in word] for word in words}\n",
    "    print(splits)\n",
    "    for word, split in splits.items():\n",
    "        while True:\n",
    "            more = False  # 是否发现可以合并的bigram\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                bigram = (split[i], split[i+1])\n",
    "                if bigram in merge_rules:\n",
    "                    token = merge_rules[bigram]\n",
    "                    split = split[:i] + [token] + split[i+2:]\n",
    "                    splits[word] = split\n",
    "                    more = True  # 如果发现有合并的bigram，说明可能还可以合并\n",
    "                else:\n",
    "                    i += 1\n",
    "                print(f\"{bigram}: {splits}\")\n",
    "            if not more:  # 如果已经没有可以合并的bigram，就可以结束当前word的合并过程了\n",
    "                break\n",
    "    tokens = [splits[word] for word in words]\n",
    "    return sum(tokens, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "60620bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', 'h', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('T', 'h'): {'This': ['Th', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Th', 'i'): {'This': ['Th', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('i', 's'): {'This': ['Th', 'is'], 'Ġis': ['Ġ', 'i', 's'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Th', 'is'): {'This': ['This'], 'Ġis': ['Ġ', 'i', 's'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Ġ', 'i'): {'This': ['This'], 'Ġis': ['Ġ', 'i', 's'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('i', 's'): {'This': ['This'], 'Ġis': ['Ġ', 'is'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Ġ', 'is'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Ġ', 'n'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('n', 'o'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('o', 't'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġ', 'a'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Ġ', 'a'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġa'], 'Ġtoken': ['Ġ', 't', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Ġ', 't'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġa'], 'Ġtoken': ['Ġt', 'o', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Ġt', 'o'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġa'], 'Ġtoken': ['Ġto', 'k', 'e', 'n'], '.': ['.']}\n",
      "('Ġto', 'k'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġa'], 'Ġtoken': ['Ġtok', 'e', 'n'], '.': ['.']}\n",
      "('Ġtok', 'e'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġa'], 'Ġtoken': ['Ġtok', 'e', 'n'], '.': ['.']}\n",
      "('e', 'n'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġa'], 'Ġtoken': ['Ġtok', 'en'], '.': ['.']}\n",
      "('Ġtok', 'en'): {'This': ['This'], 'Ġis': ['Ġis'], 'Ġnot': ['Ġ', 'n', 'o', 't'], 'Ġa': ['Ġa'], 'Ġtoken': ['Ġtoken'], '.': ['.']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = tokenize(\"This is not a token.\", merge_rules)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbc8a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "741ec54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = ['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aeea7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert actual == expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
